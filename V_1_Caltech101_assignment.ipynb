{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95067a52-5d33-4401-a05d-bfc3c2422431",
      "metadata": {
        "id": "95067a52-5d33-4401-a05d-bfc3c2422431"
      },
      "source": [
        "##  Important Rules\n",
        "**Insert team member names here:**\n",
        "\n",
        "**Rename this notebook with your MIDAS username as the prefix**\n",
        "\n",
        "**Choose \"Restart Kernel and Run All Cells ...\" under \"Kernel\" before submitting your final notebook**\n",
        "\n",
        "\n",
        "**Report your Pareto Score: Nov 25**\n",
        "\n",
        "**Due date of final: December 3rd, 11:59pm**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4974829-615c-4723-8684-1b2b02e955bc",
      "metadata": {
        "id": "f4974829-615c-4723-8684-1b2b02e955bc"
      },
      "source": [
        "## Final Project - Fine-tuning and Transfer Learning on Caltech-101 with Model Size and Accuracy Trade-offs\n",
        "\n",
        "The goal of this project is to classify images from the Caltech-101 dataset using fine-tuning and transfer learning while exploring the trade-offs between accuracy and model size. The dataset includes images from 101 categories, making it a moderately challenging classification task. Given the relatively small dataset size, we will leverage pretrained CNNs for feature extraction, comparing their performance in terms of both accuracy and model size.\n",
        "\n",
        "### Key Steps:\n",
        "#### Data Preprocessing:\n",
        "\n",
        "The Caltech-101 images will be resized, normalized, and split into training, validation, and test sets. Data augmentation techniques may be applied to mitigate overfitting.\n",
        "\n",
        "#### Transfer Learning:\n",
        "Three different pretrained models with varying sizes and complexities need to be used: each team can choose which three\n",
        "\n",
        "For each model, the convolutional layers will be frozen to use their pretrained features, and only the top fully connected layers will be fine-tuned for the Caltech-101 task. For transfer learning, all weights should be trained.\n",
        "\n",
        "#### Model Training:\n",
        "The models will be trained using the Caltech-101 dataset.\n",
        "Techniques such as learning rate scheduling, early stopping, and data augmentation can be applied.\n",
        "Each model’s training time, accuracy, and memory usage will be monitored to analyze the trade-offs.\n",
        "\n",
        "#### Trade-off Analysis:\n",
        "Using the same test dataset, the final accuracy should be computed. Each team should use the same routine to fine-tune and transfer learning three different models. And compare the accuracy vs model sizes. The teams with overall Pareto trade-off will get extra credit."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "045481e6-d891-4980-8783-473998132e3b",
      "metadata": {
        "id": "045481e6-d891-4980-8783-473998132e3b"
      },
      "source": [
        "### Details\n",
        "1. Implement dataloaders for train (70%), validation (15%) and testing (15%) of caltech101\n",
        "2. Implement visualization routines to show image, ground truth and predicted labels       \n",
        "3. Implement fine-tuning routine, using pretrained weights from Pytorch                    \n",
        "4. Implement transfer learning routine, using pretrained weights                           \n",
        "5. Use hyperparameter tuning to achieve good accuracy, using validation datasets           \n",
        "6. GPU must be used                                                                        \n",
        "7. Conduct experiment at least *3 (three) * pretrained models.                          \n",
        "8. (Extra credit) The top 3 teams with best accuracy-model size Pareto scores (<0.088) get extra credit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "769be8c5-abdb-4757-92b5-c43b3636bf43",
      "metadata": {
        "id": "769be8c5-abdb-4757-92b5-c43b3636bf43"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import v2\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision.utils as utils # Added this import\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6zPKzIdzmUDF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "6zPKzIdzmUDF",
        "outputId": "a98e6a31-7077-4df1-b7c6-9da3a4ab42ae"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5WGpWC8rH3jP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5WGpWC8rH3jP",
        "outputId": "458c793d-a473-4180-9ff9-76cd061dfcce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['CIFAR10',\n",
              " 'CIFAR100',\n",
              " 'CLEVRClassification',\n",
              " 'CREStereo',\n",
              " 'Caltech101',\n",
              " 'Caltech256',\n",
              " 'CarlaStereo',\n",
              " 'CelebA',\n",
              " 'Cityscapes',\n",
              " 'CocoCaptions',\n",
              " 'CocoDetection',\n",
              " 'Country211',\n",
              " 'DTD',\n",
              " 'DatasetFolder',\n",
              " 'EMNIST',\n",
              " 'ETH3DStereo',\n",
              " 'EuroSAT',\n",
              " 'FER2013',\n",
              " 'FGVCAircraft',\n",
              " 'FakeData',\n",
              " 'FallingThingsStereo',\n",
              " 'FashionMNIST',\n",
              " 'Flickr30k',\n",
              " 'Flickr8k',\n",
              " 'Flowers102',\n",
              " 'FlyingChairs',\n",
              " 'FlyingThings3D',\n",
              " 'Food101',\n",
              " 'GTSRB',\n",
              " 'HD1K',\n",
              " 'HMDB51',\n",
              " 'INaturalist',\n",
              " 'ImageFolder',\n",
              " 'ImageNet',\n",
              " 'Imagenette',\n",
              " 'InStereo2k',\n",
              " 'KMNIST',\n",
              " 'Kinetics',\n",
              " 'Kitti',\n",
              " 'Kitti2012Stereo',\n",
              " 'Kitti2015Stereo',\n",
              " 'KittiFlow',\n",
              " 'LFWPairs',\n",
              " 'LFWPeople',\n",
              " 'LSUN',\n",
              " 'LSUNClass',\n",
              " 'MNIST',\n",
              " 'Middlebury2014Stereo',\n",
              " 'MovingMNIST',\n",
              " 'Omniglot',\n",
              " 'OxfordIIITPet',\n",
              " 'PCAM',\n",
              " 'PhotoTour',\n",
              " 'Places365',\n",
              " 'QMNIST',\n",
              " 'RenderedSST2',\n",
              " 'SBDataset',\n",
              " 'SBU',\n",
              " 'SEMEION',\n",
              " 'STL10',\n",
              " 'SUN397',\n",
              " 'SVHN',\n",
              " 'SceneFlowStereo',\n",
              " 'Sintel',\n",
              " 'SintelStereo',\n",
              " 'StanfordCars',\n",
              " 'UCF101',\n",
              " 'USPS',\n",
              " 'VOCDetection',\n",
              " 'VOCSegmentation',\n",
              " 'VisionDataset',\n",
              " 'WIDERFace',\n",
              " '__all__',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__getattr__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_optical_flow',\n",
              " '_stereo_matching',\n",
              " 'caltech',\n",
              " 'celeba',\n",
              " 'cifar',\n",
              " 'cityscapes',\n",
              " 'clevr',\n",
              " 'coco',\n",
              " 'country211',\n",
              " 'dtd',\n",
              " 'eurosat',\n",
              " 'fakedata',\n",
              " 'fer2013',\n",
              " 'fgvc_aircraft',\n",
              " 'flickr',\n",
              " 'flowers102',\n",
              " 'folder',\n",
              " 'food101',\n",
              " 'gtsrb',\n",
              " 'hmdb51',\n",
              " 'imagenet',\n",
              " 'imagenette',\n",
              " 'inaturalist',\n",
              " 'kinetics',\n",
              " 'kitti',\n",
              " 'lfw',\n",
              " 'lsun',\n",
              " 'mnist',\n",
              " 'moving_mnist',\n",
              " 'omniglot',\n",
              " 'oxford_iiit_pet',\n",
              " 'pcam',\n",
              " 'phototour',\n",
              " 'places365',\n",
              " 'rendered_sst2',\n",
              " 'sbd',\n",
              " 'sbu',\n",
              " 'semeion',\n",
              " 'stanford_cars',\n",
              " 'stl10',\n",
              " 'sun397',\n",
              " 'svhn',\n",
              " 'ucf101',\n",
              " 'usps',\n",
              " 'utils',\n",
              " 'video_utils',\n",
              " 'vision',\n",
              " 'voc',\n",
              " 'widerface']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "dir(torchvision.datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84c143c5-7272-4013-bc21-54a6fcea6805",
      "metadata": {
        "id": "84c143c5-7272-4013-bc21-54a6fcea6805"
      },
      "source": [
        "**This is important: you want to use the common normalization vector from imageNet. Since pretrained weights are based on ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7fb50fa8-52c0-453a-bd66-ec50a2547255",
      "metadata": {
        "id": "7fb50fa8-52c0-453a-bd66-ec50a2547255"
      },
      "outputs": [],
      "source": [
        "imagenet_mean=[0.485, 0.456, 0.406]\n",
        "imagenet_std=[0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3de296b5-cfae-409b-9531-d512db0cbc72",
      "metadata": {
        "id": "3de296b5-cfae-409b-9531-d512db0cbc72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "torch version: 2.9.1+cu128\n",
            "torch.version.cuda: 12.8\n"
          ]
        }
      ],
      "source": [
        "## routine to print stats of a model\n",
        "def print_model_weight_stats(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    frozen_params = total_params - trainable_params\n",
        "\n",
        "    print(\"{}: Number of fronzen param = {:,d}, number of trainable param = {:,}\".format(type(model),frozen_params, trainable_params))\n",
        "\n",
        "## calculate the pareto value\n",
        "def compute_pareto_value(acc, msize):\n",
        "    ## accuracy must be between 0.0 and 0.9999\n",
        "    ## model size should be in millions, between 1.0 and 900!\n",
        "    if not ( 0.0 <= acc <= 0.9999):\n",
        "        raise ValueError (\"acc range must be between 0.0 and 0.9999\")\n",
        "\n",
        "    if not (1.0 <= msize <= 900):\n",
        "        raise ValueError(\"model sizse must be in millions, between 1.0M and 900M\")\n",
        "\n",
        "    xdist = 0.99 - acc\n",
        "    ydist = math.log10( np.float64(msize) )\n",
        "\n",
        "    # assumption: range of accuracy is between 0.99 to 0.80 -> delta 0.2\n",
        "    # range of model is from 1M to 100M -> delta 2 after taking log\n",
        "    # hence scale model by 0.1\n",
        "    ydist = 0.1*ydist\n",
        "\n",
        "    z_score = math.sqrt( xdist*xdist + ydist*ydist)\n",
        "    return (z_score)\n",
        "\n",
        "## use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"torch.version.cuda:\", torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2c868531-efeb-4bcb-a665-f10f31746424",
      "metadata": {
        "id": "2c868531-efeb-4bcb-a665-f10f31746424"
      },
      "outputs": [],
      "source": [
        "## routines\n",
        "\n",
        "## Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b27dce4e-90a7-4797-b277-238191e3fd7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b27dce4e-90a7-4797-b277-238191e3fd7b",
        "outputId": "5cca669c-0d7b-46a0-9395-c6539ca9cabe"
      },
      "outputs": [],
      "source": [
        "## now do some work\n",
        "## example: dataset = datasets.Caltech101(root='./data', download=True, transform=transform)\n",
        "'''transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize for models like ResNet\n",
        "    transforms.Lambda(lambda image: image.convert(\"RGB\")), # Ensure 3 channels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=imagenet_mean,\n",
        "                         std=imagenet_std)\n",
        "])\n",
        "\n",
        "dataset = datasets.Caltech101(root=\"./data\",download=True, transform=transform)\n",
        "classes = dataset.categories'''\n",
        "\n",
        "import random\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # For fully deterministic behavior:\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # For DataLoader workers\n",
        "    def _worker_init(worker_id):\n",
        "        worker_seed = seed + worker_id\n",
        "        np.random.seed(worker_seed)\n",
        "        random.seed(worker_seed)\n",
        "\n",
        "    return _worker_init\n",
        "worker_init_fn = seed_everything(42)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
        "])\n",
        "dataset = datasets.Caltech101(root='./data', download=True, transform=transform)\n",
        "classes = dataset.categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "037544fd-7e33-489e-9f0e-3f23961bd1a2",
      "metadata": {
        "id": "037544fd-7e33-489e-9f0e-3f23961bd1a2"
      },
      "outputs": [],
      "source": [
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = val_size\n",
        "train_size = len(dataset) - val_size - test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "491bc258-41a9-4f80-95c0-73dd116d80cf",
      "metadata": {
        "id": "491bc258-41a9-4f80-95c0-73dd116d80cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data shape: torch.Size([3, 224, 224]), val_data shape: torch.Size([3, 224, 224]), test_data shape: torch.Size([3, 224, 224])\n",
            "train indices: [8512, 5748, 7753, 4345, 5587, 2508, 6996, 1701, 7265, 6578], val indices: [7238, 5821, 6650, 3146, 5412, 612, 3805, 3633, 2270, 5943], test indices: [2171, 4849, 8676, 6096, 5089, 7119, 3279, 7730, 8263, 2353]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\ntransform_train = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize(mean, std),\\n])\\n\\ntransform_test = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize(mean, std),\\n])\\n\\n# Training and test sets\\ntrainset = torchvision.datasets.CIFAR100(root='./data', train=True,\\n                                         download=True, transform=transform_train)\\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize,\\n                                          shuffle=True, num_workers=2)\\n\\ntestset = torchvision.datasets.CIFAR100(root='./data', train=False,\\n                                        download=True, transform=transform_test)\\ntestloader = torch.utils.data.DataLoader(testset, batch_size=200,\\n                                         shuffle=False, num_workers=2)\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "train_indices = train_dataset.indices\n",
        "val_indices = val_dataset.indices\n",
        "test_indices = test_dataset.indices\n",
        "print(f\"data shape: {train_dataset[0][0].shape}, val_data shape: {val_dataset[0][0].shape}, test_data shape: {test_dataset[0][0].shape}\")\n",
        "print(f\"train indices: {train_indices[:10]}, val indices: {val_indices[:10]}, test indices: {test_indices[:10]}\")\n",
        "\n",
        "\n",
        "'''\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "# Training and test sets\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                         download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                        download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=200,\n",
        "                                         shuffle=False, num_workers=2)'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "922107da-0e7b-4aec-9a2b-5d82db189be1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "922107da-0e7b-4aec-9a2b-5d82db189be1",
        "outputId": "59bc7122-3fa7-4bfe-a87f-8859e0740014"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6075"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "acb95659-1196-4cf5-8139-9280f3446258",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acb95659-1196-4cf5-8139-9280f3446258",
        "outputId": "40205faa-f106-4ea5-e71d-6274dd8a8ff7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1301"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d8a275a7-68ad-42d3-8c94-b4755a16c9e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8a275a7-68ad-42d3-8c94-b4755a16c9e6",
        "outputId": "47024ab5-77d1-43cc-d047-4b76ba900904"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1301"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "20d07155-63a2-4a37-909a-183ac9dc69fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "20d07155-63a2-4a37-909a-183ac9dc69fc",
        "outputId": "18f7fa18-1842-4594-94bb-4dee865c10a0"
      },
      "outputs": [],
      "source": [
        "## data loader\n",
        "## Your code here.\n",
        "\n",
        "#DataLoaders\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=worker_init_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=worker_init_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=worker_init_fn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3xquiyJrJZ63",
      "metadata": {
        "id": "3xquiyJrJZ63"
      },
      "outputs": [],
      "source": [
        "\n",
        "def denormalize(img_tensor):\n",
        "    mean = torch.tensor(imagenet_mean, dtype=img_tensor.dtype, device=img_tensor.device).view(1, 3, 1, 1)\n",
        "    std = torch.tensor(imagenet_std, dtype=img_tensor.dtype, device=img_tensor.device).view(1, 3, 1, 1)\n",
        "    return img_tensor * std + mean\n",
        "\n",
        "def show_batch(dataloader, classes):\n",
        "    inputs, labels = next(iter(dataloader))\n",
        "    inputs = denormalize(inputs)\n",
        "    img_grid = utils.make_grid(inputs, nrow=4)\n",
        "    npimg = img_grid.numpy()\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Ground Truth: \" + \", \".join([classes[l] for l in labels]))\n",
        "    plt.show()\n",
        "\n",
        "#show_batch(test_loader, classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0594c9c7-890c-4a67-8740-1a517cd01e0c",
      "metadata": {
        "id": "0594c9c7-890c-4a67-8740-1a517cd01e0c"
      },
      "source": [
        "#### Fine tuning, use ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "eaa7b550-250b-4ac7-b0a4-c7e51a14ffa4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaa7b550-250b-4ac7-b0a4-c7e51a14ffa4",
        "outputId": "c473adee-1d33-45e9-8d9e-2acb718c8cc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torchvision.models.resnet.ResNet'>: Number of fronzen param = 11,176,512, number of trainable param = 51,813\n"
          ]
        }
      ],
      "source": [
        "## Your code\n",
        "#Load Pretrained ResNet18\n",
        "\n",
        "\n",
        "frozen_resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "for param in frozen_resnet18.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# set up unfrozen final layer\n",
        "frozen_resnet18.fc = nn.Linear(frozen_resnet18.fc.in_features, len(classes))\n",
        "frozen_resnet18 = frozen_resnet18.to(device)\n",
        "\n",
        "# Print stats after fine-tuning setup\n",
        "print_model_weight_stats(frozen_resnet18)\n",
        "frozen_resnet18 = frozen_resnet18.to(device)\n",
        "\n",
        "#Loss & Optimizer\n",
        "# -----------------------------\n",
        "def get_loss_and_opt(model):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    return criterion, optimizer\n",
        "\n",
        "criterion, optimizer = get_loss_and_opt(frozen_resnet18)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a-ayHby1Yhwg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-ayHby1Yhwg",
        "outputId": "c7ec58d9-a742-4a9a-8b93-208ea22c0e77"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b707576b-7d8f-4ce9-b039-1ca832445ec6",
      "metadata": {
        "id": "b707576b-7d8f-4ce9-b039-1ca832445ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01/40 | train loss 0.2469 acc 96.51% | val loss 0.3690 acc 90.55% \n",
            "  ↳ Saved new best to fres18_best.pt\n",
            "no improvement for 1 epochs\n",
            "Epoch 02/40 | train loss 0.2316 acc 96.71% | val loss 0.3731 acc 90.47% \n",
            "no improvement for 2 epochs\n",
            "Epoch 03/40 | train loss 0.2260 acc 96.99% | val loss 0.3694 acc 90.39% \n",
            "no improvement for 3 epochs\n",
            "Epoch 04/40 | train loss 0.2166 acc 96.89% | val loss 0.3590 acc 90.78% \n",
            "  ↳ Saved new best to fres18_best.pt\n",
            "no improvement for 4 epochs\n",
            "Epoch 05/40 | train loss 0.2108 acc 96.84% | val loss 0.3582 acc 90.55% \n",
            "  ↳ Saved new best to fres18_best.pt\n",
            "no improvement for 5 epochs\n",
            "Early stopping triggered after 5 epochs.\n"
          ]
        }
      ],
      "source": [
        "## Your code\n",
        "import torch.nn.functional as F\n",
        "\n",
        "num_classes = 101  # Caltech101 has 101 categories\n",
        "\n",
        "mixup_or_cutmix = v2.RandomChoice([\n",
        "    v2.MixUp(num_classes=num_classes, alpha=0.2),\n",
        "    v2.CutMix(num_classes=num_classes, alpha=1.0),\n",
        "])\n",
        "\n",
        "def ce_loss(logits, target):\n",
        "    if target.dtype in (torch.long, torch.int64):  # hard labels\n",
        "        return F.cross_entropy(logits, target)\n",
        "    # soft labels: target shape [N, C]\n",
        "    log_prob = F.log_softmax(logits, dim=1)\n",
        "    return -(target * log_prob).sum(dim=1).mean()\n",
        "\n",
        "#Training\n",
        "def accuracy(logits, targets):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "def run_epoch(loader, model, optimizer, criterion, device, train_mode: bool = True):\n",
        "    model.train(train_mode)\n",
        "    model.to(device)\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    # n is the number of the samples\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if train_mode:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "        bs = y.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy(logits, y) * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, save_path, epochs=5, scheduler=None, hard_aug=False, min_delta=0.001, patience=5):\n",
        "    best_val_loss = 999999.99\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for ep in range(1, 1 + epochs):\n",
        "        model.train(True)\n",
        "        total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "            if hard_aug:\n",
        "                x, y_mixed = mixup_or_cutmix(x, y) ## changed\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            logits = model(x)\n",
        "            if hard_aug:\n",
        "                loss = ce_loss(logits, y_mixed)  ## changed!\n",
        "            else:\n",
        "                loss = ce_loss(logits, y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            total_loss += loss.item() * bs\n",
        "            total_acc  += accuracy(logits, y) * bs\n",
        "            n += bs\n",
        "\n",
        "        train_loss = total_loss / n\n",
        "        train_acc = total_acc / n\n",
        "        val_loss,   val_acc   = run_epoch(val_loader, model,  optimizer, criterion, device, train_mode=False)\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {ep:02d}/{epochs} | \"\n",
        "            f\"train loss {train_loss:.4f} acc {train_acc*100:.2f}% | \"\n",
        "            f\"val loss {val_loss:.4f} acc {val_acc*100:.2f}% \")\n",
        "        # Save the best model by validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            torch.save({\"model_state\": model.state_dict(),}, save_path)\n",
        "            print(f\"  ↳ Saved new best to {save_path}\")\n",
        "\n",
        "        if val_loss < best_val_loss - min_delta:\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"no improvement for {epochs_no_improve} epochs\")\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping triggered after {ep} epochs.\")\n",
        "            break\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "        \n",
        "\n",
        "save_path = \"fres18_best.pt\"\n",
        "train_model(frozen_resnet18, train_loader, val_loader, criterion, optimizer, save_path, epochs=40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa22679c-217f-482e-9f56-37649efa274b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "aa22679c-217f-482e-9f56-37649efa274b",
        "outputId": "32eb4de3-0cb6-4a2d-a656-f7b770b61ce1"
      },
      "outputs": [],
      "source": [
        "## testing loss\n",
        "\n",
        "#Testing routine\n",
        "def run_test_dataset(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "run_test_dataset(frozen_resnet18, test_loader)\n",
        "\n",
        "\n",
        "#Visualization pediction\n",
        "\n",
        "\n",
        "def visualize_predictions(model, dataset, device, n_rows: int = 2, n_cols: int = 3):\n",
        "    \"\"\"\n",
        "    Run inference on the first n_samples of a dataset and plot them\n",
        "    with ground-truth (GT) and predicted (Pred) labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : torch.nn.Module\n",
        "        Trained PyTorch model.\n",
        "    dataset : torch.utils.data.Dataset\n",
        "        Dataset providing (image, label) pairs.\n",
        "    device : torch.device\n",
        "        Device to run inference on.\n",
        "    n_rows : int, optional\n",
        "        Number of rows, default = 2.\n",
        "    n_cols : int, optional\n",
        "        Number of cols, default = 3\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    n_samples = n_rows * n_cols\n",
        "\n",
        "    ### Your code here\n",
        "    inputs, labels = [], []\n",
        "    for i in range(n_samples):\n",
        "        input, label = dataset[i]\n",
        "        inputs.append(input)\n",
        "        labels.append(label)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_batch = torch.stack(inputs)\n",
        "        x_batch = x_batch.to(device)\n",
        "        logits = model(x_batch)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    # Denormalize and move to CPU\n",
        "    x_batch_cpu = denormalize(x_batch.cpu())\n",
        "\n",
        "    plt.figure(figsize=(n_cols * 3, n_rows * 3))\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(n_rows, n_cols, i + 1)\n",
        "        img = x_batch_cpu[i]\n",
        "        img = img.permute(1,2,0).numpy()\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"GT: {labels[i]}, Pred: {preds[i]}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions(frozen_resnet18, test_dataset, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5304da55-2bcd-4003-968e-90af9c0195b6",
      "metadata": {
        "id": "5304da55-2bcd-4003-968e-90af9c0195b6"
      },
      "outputs": [],
      "source": [
        "## do a quick plot\n",
        "## inputs, labels = next(iter(test_loader))\n",
        "## your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "705e78f8-a717-4f93-8783-702cd2624242",
      "metadata": {
        "id": "705e78f8-a717-4f93-8783-702cd2624242"
      },
      "outputs": [],
      "source": [
        "## ResNet18, transfer learning\n",
        "## code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf9f33a-cf59-4f68-bb0c-4b9220f69557",
      "metadata": {
        "id": "eaf9f33a-cf59-4f68-bb0c-4b9220f69557"
      },
      "outputs": [],
      "source": [
        "## testing loss\n",
        "run_test_dataset(model18t, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f299a228-37dd-413b-be42-b0673688beaf",
      "metadata": {
        "id": "f299a228-37dd-413b-be42-b0673688beaf"
      },
      "outputs": [],
      "source": [
        "## do a quick plot\n",
        "## Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50193b2b-2443-4d98-b5ef-4059d84339a4",
      "metadata": {
        "id": "50193b2b-2443-4d98-b5ef-4059d84339a4"
      },
      "source": [
        "#### ShuffleNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfc4337e-2415-4d88-8122-3d07a7ac4612",
      "metadata": {
        "id": "cfc4337e-2415-4d88-8122-3d07a7ac4612"
      },
      "outputs": [],
      "source": [
        "## fine tuning shuffle Net\n",
        "## repeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eb7a827-34da-48bf-9c31-1c2dfe0d6fa0",
      "metadata": {
        "id": "7eb7a827-34da-48bf-9c31-1c2dfe0d6fa0"
      },
      "outputs": [],
      "source": [
        "## testing loss\n",
        "run_test_dataset(modelsnf, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da0a5c7b-439a-4095-b42e-7200e4cd5a3f",
      "metadata": {
        "id": "da0a5c7b-439a-4095-b42e-7200e4cd5a3f"
      },
      "outputs": [],
      "source": [
        "## do a quick plot\n",
        "## your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc6ead38-4354-4d30-b6a7-99fe4a014f0b",
      "metadata": {
        "id": "dc6ead38-4354-4d30-b6a7-99fe4a014f0b"
      },
      "outputs": [],
      "source": [
        "## transfer learning of shuffleNet\n",
        "##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c879b0d-12f1-4baa-ab43-90732c958943",
      "metadata": {
        "id": "5c879b0d-12f1-4baa-ab43-90732c958943"
      },
      "outputs": [],
      "source": [
        "## testing loss\n",
        "run_test_dataset(modelsnt, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0217a99-5b60-4a3e-8194-3356a7a5677d",
      "metadata": {
        "id": "f0217a99-5b60-4a3e-8194-3356a7a5677d"
      },
      "outputs": [],
      "source": [
        "## do a quick plot\n",
        "## your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac4ecf3-0cc8-40d3-8568-8f4957a611e0",
      "metadata": {
        "id": "3ac4ecf3-0cc8-40d3-8568-8f4957a611e0"
      },
      "source": [
        "#### You have to implement the third model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b9dda0-2269-4b21-a1f4-83c32ac0feef",
      "metadata": {
        "id": "57b9dda0-2269-4b21-a1f4-83c32ac0feef"
      },
      "outputs": [],
      "source": [
        "## add lots of code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd47e72b-3b9c-4205-a05b-ec2a1b7cff05",
      "metadata": {
        "id": "fd47e72b-3b9c-4205-a05b-ec2a1b7cff05"
      },
      "source": [
        "#### Report the Pareto value here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d05fa221-ef75-49e1-a0af-354ab056d59c",
      "metadata": {
        "id": "d05fa221-ef75-49e1-a0af-354ab056d59c"
      },
      "outputs": [],
      "source": [
        "## for ResNet18, testing accuracy is for transfer learning\n",
        "acc = 0.9347\n",
        "\n",
        "## the model size is copied from print command\n",
        "msize = 11228325.0/1000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f07c2820-2f8c-4238-80e4-97e82279a1df",
      "metadata": {
        "id": "f07c2820-2f8c-4238-80e4-97e82279a1df"
      },
      "outputs": [],
      "source": [
        "print(\"ResNet Pareto score is {:.4f}\".format( compute_pareto_value(acc, msize)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb01caf6-6514-4dc1-a4eb-a85c950760bc",
      "metadata": {
        "id": "fb01caf6-6514-4dc1-a4eb-a85c950760bc"
      },
      "outputs": [],
      "source": [
        "## for shufflenet\n",
        "acc = 0.9032\n",
        "msize = 1357129.0/1000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ada069-0274-44ab-aaab-f54a0d32b75f",
      "metadata": {
        "id": "85ada069-0274-44ab-aaab-f54a0d32b75f"
      },
      "outputs": [],
      "source": [
        "print(\"ShuffleNet pareto score is {:.4f}\".format( compute_pareto_value(acc, msize)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bac2516-c6d2-4a2e-9ab7-0ba56feb2cdf",
      "metadata": {
        "id": "6bac2516-c6d2-4a2e-9ab7-0ba56feb2cdf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "V-1-Caltech101_assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
